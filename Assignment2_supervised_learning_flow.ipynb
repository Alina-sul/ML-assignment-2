{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec202ada",
   "metadata": {},
   "source": [
    "# Assignment2 - Supervised Learning flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a2810",
   "metadata": {},
   "source": [
    "# Part 1 - Student details:\n",
    "* Please write the First Name and last 4 digits of the i.d. for each student. For example:\n",
    "<pre>Israel 9812</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student 1: Alina 5247"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67c2ee-87c8-499c-a04f-1853c332f51d",
   "metadata": {},
   "source": [
    "## Part 2 - Initial Preparations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eb52c5-242a-4b3f-a95f-82ce1a4deb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564f41b-7256-44ab-8e28-8905affa950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e27610-b640-4db0-80c9-789b5f5fae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "wine_train = pd.read_csv('./wine_train.csv')\n",
    "wine_test = pd.read_csv('./wine_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bfffb7-4a72-407b-bdc3-9c13b92393dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First five rows of the training set\n",
    "print(\"First five rows of the training set:\")\n",
    "display(wine_train.head())\n",
    "\n",
    "# First five rows of the test set\n",
    "print(\"First five rows of the test set:\")\n",
    "display(wine_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da766c-bf6c-413e-8fe1-cb3364b9f90c",
   "metadata": {},
   "source": [
    "#### Data Set Charachtiristics \n",
    "An overview of the datasetâ€™s structure and contents, highlighting key features and patterns to guide further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b9f67-cf06-424d-b97b-4aa2565105f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in training data\n",
    "print(\"Missing values in training data:\")\n",
    "print(wine_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd398f-dce4-426c-a197-47c73a00fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83955cb8-0030-41ad-8807-6af8ce3e95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c92660-afc7-4dbe-bcb3-caa20df621b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Unique Values per Feature\n",
    "for col in wine_train.columns:\n",
    "    print(f\"{col}: {wine_train[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c671c-40e1-428e-a09f-d60479c045e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Duplicate Records\n",
    "duplicate_rows = wine_train.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba597b7-448d-462e-b5f5-7f4489359d5f",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "* <b>Training and Test Sets:</b> Both sets contain 14 features related to the chemical properties of wines and a target variable indicating the wine class.\n",
    "* The first five rows show diverse values across features like alcohol, malic acid, ash, and proline.\n",
    "* Significant range and variation in feature values suggest a rich dataset suitable for modeling.\n",
    "* No duplicate rows found in the training set, ensuring data uniqueness and integrity.\n",
    "#### Key Features: \n",
    "* <b>Chemical Properties:</b> Features such as alcohol content, malic acid, ash etc.\n",
    "* <b>Target Variable:</b> The target column classifies the wine into different classes, essential for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b724f-786e-4241-ac70-25c8d3f54558",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda70618-2541-42e1-8fdd-6f6bfbe14d9f",
   "metadata": {},
   "source": [
    "#### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6131838-daa7-465e-98d2-67dc6325dd08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get basic statistics of the training data\n",
    "print(\"Basic statistics of the training data:\")\n",
    "display(wine_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf7ea8-3cd3-485f-a109-cb4819df9498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get basic statistics of the testing data\n",
    "print(\"Basic statistics of the testing data:\")\n",
    "display(wine_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad6b8d-7cd2-4e19-9113-5bd05d24a5d1",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "While most features exhibit stable distributions, certain variables like proline and malic acid indicate potential skewness and outliers due to their wide ranges and mean values significantly higher than the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cea0e9-3aa1-4162-befd-4f71bef7ef2b",
   "metadata": {},
   "source": [
    "#### Class Distribution\n",
    "A necessary initial check to ensure the target distribution is balanced and appropriate for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1036e-4b51-43f0-9340-f0c08b1c0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='target', data=wine_train)\n",
    "plt.title('Distribution of Wine Classes')\n",
    "plt.xlabel('Wine Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b54b9a3-92a6-4d8d-8405-1e624dd6498f",
   "metadata": {},
   "source": [
    "### Features Correlation Matrix\n",
    "Identifying how features are linearly related to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b775a7b-c6b7-4059-8280-171d44194049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude 'target' from correlation matrix\n",
    "features = wine_train.drop('target', axis=1)\n",
    "corr_matrix = features.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Draw the heatmap with the mask \n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    mask=mask,\n",
    "    cmap='coolwarm',\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": .5},\n",
    "    xticklabels=corr_matrix.columns,\n",
    "    yticklabels=corr_matrix.columns\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d8ca8-680f-483e-9a7b-1b6ebb6d4572",
   "metadata": {},
   "source": [
    "#### HeatMap Overview:\n",
    "* High correlation between total_phenols and flavanoids (0.87), indicating redundancy.\n",
    "* Correlation between od280/od315_of_diluted_wines and flavanoids (0.78) suggests similar information is captured by both features.\n",
    "* Low correlation features like malic_acid and nonflavanoid_phenols could provide unique information, warranting further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7eb1c2-865c-41ea-80d1-8f8efaf40b21",
   "metadata": {},
   "source": [
    "#### Further Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d440ed4-74c2-4ab6-8d20-d1cff46d33fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a correlation threshold (e.g., 0.75) to detect highly correlated features\n",
    "correlation_threshold = 0.75\n",
    "\n",
    "# Mask the upper triangle to avoid duplicate pairs\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Apply the mask to the correlation matrix\n",
    "masked_corr_matrix = corr_matrix.mask(mask)\n",
    "\n",
    "# Filter for pairs with correlation higher than the threshold\n",
    "high_corr_pairs = masked_corr_matrix.stack().reset_index()\n",
    "high_corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs['Correlation'].abs() > correlation_threshold]\n",
    "\n",
    "# Print the high correlation pairs\n",
    "print(high_corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f65cb8-7ec0-4d70-81c4-2f924c981b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for highly correlated features (adjust based on findings from heatmap)\n",
    "sns.pairplot(wine_train, vars=['total_phenols', 'flavanoids', 'od280/od315_of_diluted_wines'], hue='target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f73f1-b59b-43f4-bb37-8ffbd693eb5b",
   "metadata": {},
   "source": [
    "#### Pairplot Overview\n",
    "<b> Separation of Classes:</b>\n",
    "* Class 0 (light color) has generally lower values for total phenols and flavanoids compared to Class 1 and Class 2.\n",
    "* Class 2 (dark color) tends to have higher values in both total phenols and od280/od315_of_diluted_wines.\n",
    "\n",
    "<b> High Correlation:</b>\n",
    "* The scatterplots show a very strong linear relationship between total phenols and flavanoids (high positive correlation), as well as between flavanoids and od280/od315_of_diluted_wines.\n",
    "* This confirms what we saw in the heatmap: these features are highly correlated and may be redundant if used together in a model. This could lead to multicollinearity, which might cause issues in linear models like Logistic Regression.\n",
    "\n",
    "<b>Class Overlap:</b>\n",
    "There is some overlap between the classes, especially between Class 1 and Class 2 for the feature flavanoids. This might mean that some models could have difficulty separating these classes purely based on this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532764a3-a288-47df-ab1a-45e1fc69fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution and skewness for important features\n",
    "wine_train[features.columns].hist(bins=15, figsize=(15, 10), layout=(4, 4))\n",
    "plt.show()\n",
    "\n",
    "# Calculate skewness to see if any feature is heavily skewed\n",
    "skewness = wine_train[features.columns].skew()\n",
    "print(\"Skewness:\\n\", skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe0a98-3e6d-47b1-9d3b-d73a8c348440",
   "metadata": {},
   "source": [
    "#### <b>Overview:</b>\n",
    "\n",
    "* <b> Normal Distribution:</b>\n",
    "\n",
    "Features like alcohol, ash, total phenols, and od280/od315_of_diluted_wines have relatively symmetric distributions, indicating they are approximately normally distributed.\n",
    "\n",
    "* <b> Skewed Distributions:</b>\n",
    "\n",
    "Malic acid and proline are positively skewed (right-skewed). The malic acid distribution is heavily concentrated around lower values, while proline has a long tail, meaning most of the data is concentrated on lower values but with some extreme higher values.\n",
    "Magnesium, color intensity, and proanthocyanins also show positive skewness.\n",
    "Hue is slightly negatively skewed, but its distribution is more balanced compared to the others.\n",
    "\n",
    "* <b> Highly Skewed Features: </b>\n",
    "\n",
    "Proline has the highest skewness (0.81), followed by magnesium (0.79), and color intensity (0.78). These features are heavily right-skewed, meaning that a large portion of the data is concentrated in lower values, but there are some extreme higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa4728-bc2c-4f4a-be6a-8a557026f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot to identify outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=wine_train[features.columns])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50464c8c-7413-40db-9bef-14ad1e55c425",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "Proline stands out with a much larger scale compared to the other features. This difference in scale could potentially affect the model's performance, particularly in distance-based models like KNN or in gradient-based algorithms (like logistic regression or neural networks).\n",
    "Other features, such as magnesium, have values that are in a more reasonable range but also have outliers, which might affect the model's robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36672a",
   "metadata": {},
   "source": [
    "## Part 3 - Experiments\n",
    "You could add as many code cells as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ac3e3-d40e-4fd5-9e91-e934d639e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# After finiding high correlation between some features - \n",
    "# checking the importance of each to consider removing some later on\n",
    "\n",
    "# Prepare Data\n",
    "X = wine_train.drop('target', axis=1)\n",
    "y = wine_train['target']\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feature_importances.sort_values(ascending=False, inplace=True)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773f1e4-3063-434f-8b76-9e7457ef1626",
   "metadata": {},
   "source": [
    "#### Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097afb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable from training data\n",
    "X_train = wine_train.drop('target', axis=1)\n",
    "y_train = wine_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099e389-0188-4b2f-baad-ed7abe54ee43",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7098585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline for combining different feature engineering techniques\n",
    "# The pipeline allows us to apply multiple transformations sequentially\n",
    "\n",
    "# 1. StandardScaler for normalization\n",
    "# 2. PolynomialFeatures to add interactions and higher-degree terms (degree=2 in this case)\n",
    "# 3. PCA for dimensionality reduction, keeping 95% of the variance\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Normalization step\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),  # Adding polynomial features\n",
    "    ('pca', PCA(n_components=0.95))  # Dimensionality reduction step, keeping 95% of variance\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_transformed = pipeline.fit_transform(X_train)\n",
    "print(f'Transformed shape: {X_transformed.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b327e4-122d-41c7-a9bd-8fceda8565fc",
   "metadata": {},
   "source": [
    "#### Grid Search with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d16473-4a8f-4e95-8b56-60b638b43c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define models and parameters\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "# Define hyperparameters for each model\n",
    "param_grid = {\n",
    "    # Logistic Regression hyperparameters\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength: smaller values specify stronger regularization\n",
    "        'solver': ['liblinear', 'lbfgs', 'saga'],  # Different solvers to optimize the logistic function\n",
    "        'max_iter': [10000]  # Maximum number of iterations for solver convergence\n",
    "    },\n",
    "    \n",
    "    # Random Forest hyperparameters\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200, 500],  # Number of trees in the forest\n",
    "        'max_depth': [None, 10, 20, 30],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "        'bootstrap': [True, False]  # Whether to use bootstrap samples when building trees\n",
    "    },\n",
    "    \n",
    "    # K-Nearest Neighbors hyperparameters\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors to consider (k)\n",
    "        'weights': ['uniform', 'distance'],  # Weight function used in prediction (uniform: equal, distance: weighted)\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm to compute the nearest neighbors\n",
    "        'p': [1, 2]  # Power parameter for the Minkowski metric (p=1: Manhattan, p=2: Euclidean)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13921fd7-4835-4b52-a441-d3865612377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a scorer using macro-average F1 score\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Run Grid Search with 5-fold cross-validation for Random Forest\n",
    "grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(), \n",
    "                              param_grid=param_grid['Random Forest'], \n",
    "                              scoring=scorer, \n",
    "                              cv=5)\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Run Grid Search for Logistic Regression\n",
    "grid_search_lr = GridSearchCV(estimator=LogisticRegression(), \n",
    "                              param_grid=param_grid['Logistic Regression'], \n",
    "                              scoring=scorer, \n",
    "                              cv=5)\n",
    "\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# Run Grid Search for KNN\n",
    "grid_search_knn = GridSearchCV(estimator=KNeighborsClassifier(), \n",
    "                               param_grid=param_grid['KNN'], \n",
    "                               scoring=scorer, \n",
    "                               cv=5)\n",
    "\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Collect the results into a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Logistic Regression', 'KNN'],\n",
    "    'Best Macro F1 Score': [\n",
    "        grid_search_rf.best_score_,\n",
    "        grid_search_lr.best_score_,\n",
    "        grid_search_knn.best_score_\n",
    "    ],\n",
    "    'Best Params': [\n",
    "        grid_search_rf.best_params_,\n",
    "        grid_search_lr.best_params_,\n",
    "        grid_search_knn.best_params_\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"All Model Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d97f11",
   "metadata": {},
   "source": [
    "## Part 4 - Training \n",
    "Use the best combination of feature engineering, model (algorithm and hyperparameters) from the experiment part (part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93713ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model with the highest score\n",
    "best_model_idx = results_df['Best Macro F1 Score'].idxmax()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "best_model_params = results_df.loc[best_model_idx, 'Best Params']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Parameters: {best_model_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065efed-44a4-4c16-a6e6-9d705665c35a",
   "metadata": {},
   "source": [
    "#### Retrain With The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1ce35-a7d2-49c0-b9fd-c83bdeb153b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine_train.drop(columns=['target'])\n",
    "y = wine_train.target\n",
    "X_test = wine_test.drop(columns=['target'])\n",
    "y_test = wine_test.target\n",
    "\n",
    "# Use the pipeline from before\n",
    "X_train_transformed = pipeline.fit_transform(X)  \n",
    "X_test_transformed = pipeline.transform(X_test) \n",
    "\n",
    "print(f\"Transformed train shape: {X_train_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the best model on the full training set\n",
    "if best_model_name == 'Random Forest':\n",
    "    best_model = RandomForestClassifier(**best_model_params)\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    best_model = LogisticRegression(**best_model_params)\n",
    "elif best_model_name == 'KNN':\n",
    "    best_model = KNeighborsClassifier(**best_model_params)\n",
    "\n",
    "# Train the best model on the transformed train data\n",
    "best_model.fit(X_train_transformed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ab902",
   "metadata": {},
   "source": [
    "## Part 5 - Apply on test and show model performance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9971aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the transformed test set\n",
    "y_test_pred = best_model.predict(X_test_transformed)\n",
    "\n",
    "# Show the first 5 predictions along with actual labels\n",
    "print(\"First 5 Predictions vs Actual Labels:\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {y_test_pred[i]}, Actual: {y_test.iloc[i]}\")\n",
    "\n",
    "# Evaluate the model on the test set using the macro-average F1 score\n",
    "f1_test = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f'\\nMacro F1 Score on the Test Set: {f1_test:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f16b29-8f9e-465b-80fc-2e2416c950c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af7f08-8661-4223-a7b1-74a00030a673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate and plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d74025-ff25-438c-b713-b4e70dd90123",
   "metadata": {},
   "source": [
    "### <b> Assignment Conclusion: </b>\n",
    "\n",
    "The project successfully classified wine types with a high degree of accuracy using a Random Forest model. Feature scaling and hyperparameter tuning were crucial in improving model performance. The high Macro F1 Score and classification report metrics demonstrate that the model can generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df9f19-0581-4b26-8ea9-b70b266d6598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
